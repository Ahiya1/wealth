# Explorer 2 Report: Technology Patterns & Dependencies - Iteration 15

## Executive Summary

Iteration 15 builds on a solid foundation from Iteration 14: all core export utilities (CSV, Excel, JSON, ZIP, AI context, README) are implemented and tested. The focus shifts to integration patterns for the complete ZIP export package, Vercel Blob Storage caching, ExportHistory CRUD operations, and Vercel Cron cleanup jobs. Key finding: archiver is already installed and working well, @vercel/blob needs to be added, and the existing tRPC patterns provide a strong blueprint for the new export endpoints needed in this iteration.

## Discoveries

### Iteration 14 Foundation Review (What's Already Built)

**Export Utilities - COMPLETE**
- `src/lib/csvExport.ts` - All 6 data types supported (transactions, budgets, goals, accounts, recurring, categories)
- `src/lib/xlsxExport.ts` - Excel generation for all 6 data types, tested and working
- `src/lib/aiContextGenerator.ts` - AI context JSON with field descriptions, category hierarchy, prompts
- `src/lib/archiveExport.ts` - ZIP package creation using archiver library
- `src/lib/readmeGenerator.ts` - Dynamic README with record counts, date ranges, AI prompt templates

**Export Router - COMPLETE**
- `src/server/api/routers/exports.router.ts` - 6 tRPC endpoints (one per data type)
- Format switching: CSV/JSON/EXCEL via input enum
- Base64 encoding for binary transport
- Record count and file size tracking
- Already registered in `src/server/api/root.ts`

**Database Schema - COMPLETE**
- ExportHistory model added to `prisma/schema.prisma`
- Migration applied successfully (20251109 timestamp)
- Indexes on userId, createdAt, expiresAt
- Enums: ExportType, ExportFormat, ExportDataType

**Testing Infrastructure - COMPLETE**
- `src/lib/__tests__/csvExport.test.ts` - CSV UTF-8 BOM validation
- `src/lib/__tests__/xlsxExport.test.ts` - Excel file validity tests
- `src/lib/__tests__/aiContextGenerator.test.ts` - AI context schema validation
- `src/lib/__tests__/archiveExport.test.ts` - ZIP structure validation
- `src/lib/__tests__/readmeGenerator.test.ts` - README template tests

### Iteration 15 Requirements (What Needs Building)

**1. Complete Export ZIP Package Endpoint**
- New tRPC procedure: `exports.exportComplete`
- Fetches all 6 data types in parallel (Promise.all pattern)
- Generates 9 files: 6 CSVs + README.md + ai-context.json + summary.json
- Creates ZIP using archiveExport.ts utility
- Returns ZIP blob with metadata
- Records to ExportHistory with format: 'ZIP', exportType: 'COMPLETE'

**2. Vercel Blob Storage Integration**
- Install `@vercel/blob` package (NEW DEPENDENCY)
- Environment variable: `BLOB_READ_WRITE_TOKEN` (from Vercel dashboard)
- Upload pattern: `put(path, buffer, options)` returns blob URL
- Download pattern: Return blob.url to client (presigned, CDN-cached)
- Delete pattern: `del(blobKey)` for cleanup

**3. Export History CRUD Operations**
- New tRPC procedures:
  - `exports.getExportHistory` - List last 10 exports for user
  - `exports.redownloadExport` - Fetch cached export from Blob Storage
  - `exports.deleteExport` - Manual delete (optional, mainly for cleanup cron)
- Query optimization: Use indexes on userId, createdAt
- Date formatting: Display createdAt as "Nov 9, 2025 at 8:30 PM"

**4. 30-Day Cleanup Cron Job**
- New API route: `src/app/api/cron/cleanup-exports/route.ts`
- Vercel Cron configuration in `vercel.json`
- Schedule: Daily at 2 AM UTC (`0 2 * * *`)
- Authentication: CRON_SECRET verification (already configured)
- Logic: Find exports where expiresAt < now(), delete from Blob, delete from DB
- Logging: Track deleted count and freed bytes

### Technology Stack - Iteration 15

**Already Installed (Iteration 14)**
- archiver@7.0.1 - ZIP generation (working perfectly)
- @types/archiver@7.0.0 - TypeScript support
- xlsx@0.18.5 - Excel generation
- date-fns@3.6.0 - Date formatting
- @prisma/client@5.22.0 - Database ORM

**New Dependency (Iteration 15)**
- `@vercel/blob@latest` - Cloud storage for export caching

**Installation Commands:**
```bash
npm install @vercel/blob
```

**Environment Variables (Add to .env):**
```bash
# Vercel Blob Storage (get from Vercel Dashboard → Storage → Create Blob Store)
BLOB_READ_WRITE_TOKEN="vercel_blob_rw_..."

# Already configured (verify exists)
CRON_SECRET="..."  # For cleanup cron job authentication
```

**Vercel Configuration (Add to vercel.json):**
```json
{
  "crons": [
    {
      "path": "/api/cron/generate-recurring",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/cleanup-exports",
      "schedule": "0 2 * * *"
    }
  ]
}
```

### Export Package Structure (Complete ZIP)

**Folder Layout:**
```
wealth-export-2025-11-09/
├── README.md                      (Generated by readmeGenerator.ts)
├── ai-context.json                (Generated by aiContextGenerator.ts)
├── summary.json                   (Export metadata: date, counts, user info)
├── transactions.csv               (All transactions - from csvExport.ts)
├── recurring-transactions.csv     (Recurring templates - from csvExport.ts)
├── budgets.csv                    (Monthly budgets - from csvExport.ts)
├── goals.csv                      (Financial goals - from csvExport.ts)
├── accounts.csv                   (Account balances - from csvExport.ts)
└── categories.csv                 (Category hierarchy - from csvExport.ts)
```

**summary.json Structure:**
```json
{
  "exportVersion": "1.0",
  "exportedAt": "2025-11-09T20:30:00Z",
  "user": {
    "email": "user@example.com",
    "currency": "NIS",
    "timezone": "America/New_York"
  },
  "recordCounts": {
    "transactions": 1247,
    "recurringTransactions": 8,
    "budgets": 12,
    "goals": 3,
    "accounts": 4,
    "categories": 45
  },
  "dateRange": {
    "earliest": "2024-01-01",
    "latest": "2025-11-09"
  },
  "fileSize": 2458932,
  "format": "ZIP"
}
```

## Patterns Identified

### Pattern 1: Vercel Blob Storage Upload/Download

**Description:** Upload generated exports to Vercel Blob Storage for 30-day caching, return presigned URL for downloads.

**Use Case:** Complete export package caching, re-download functionality.

**Implementation:**
```typescript
import { put, del } from '@vercel/blob'

// Upload export after generation
async function cacheExport(zipBuffer: Buffer, userId: string): Promise<string> {
  const timestamp = format(new Date(), 'yyyy-MM-dd-HH-mm-ss')
  const path = `exports/${userId}/complete-${timestamp}.zip`
  
  const blob = await put(path, zipBuffer, {
    access: 'public',  // Presigned URL accessible without auth
    addRandomSuffix: false,  // Keep clean filenames
    contentType: 'application/zip'
  })
  
  return blob.url  // Store this in ExportHistory.blobKey
}

// Download (re-download from history)
async function getExport(blobKey: string) {
  // Simply return the blob.url - Vercel Blob provides CDN-cached access
  // Client fetches directly from blob.url (no server download needed)
  return { downloadUrl: blobKey }
}

// Cleanup (cron job)
async function deleteExpiredExport(blobKey: string) {
  await del(blobKey)
}
```

**Recommendation:** Use this pattern in `exports.exportComplete` and cleanup cron job.

### Pattern 2: Parallel Data Fetching for Complete Export

**Description:** Fetch all 6 data types simultaneously using Promise.all() to minimize database roundtrips.

**Use Case:** Complete export generation (fetch transactions, budgets, goals, accounts, recurring, categories in parallel).

**Implementation (from existing users.router.ts pattern):**
```typescript
const [transactions, budgets, goals, accounts, recurringTransactions, categories] = await Promise.all([
  ctx.prisma.transaction.findMany({
    where: { userId: ctx.user.id },
    include: { category: true, account: true },
    orderBy: { date: 'desc' },
    take: 10000  // Prevent memory overflow
  }),
  ctx.prisma.budget.findMany({
    where: { userId: ctx.user.id },
    include: { category: true },
    orderBy: { month: 'desc' }
  }),
  ctx.prisma.goal.findMany({
    where: { userId: ctx.user.id },
    include: { linkedAccount: true },
    orderBy: { createdAt: 'desc' }
  }),
  ctx.prisma.account.findMany({
    where: { userId: ctx.user.id },
    orderBy: { name: 'asc' }
  }),
  ctx.prisma.recurringTransaction.findMany({
    where: { userId: ctx.user.id },
    include: { category: true, account: true },
    orderBy: { nextScheduledDate: 'asc' }
  }),
  ctx.prisma.category.findMany({
    where: { userId: ctx.user.id },
    include: { parent: true },
    orderBy: { name: 'asc' }
  })
])
```

**Recommendation:** Use this exact pattern in `exports.exportComplete` endpoint.

### Pattern 3: ExportHistory Tracking

**Description:** Record every export to database with metadata for analytics and re-download functionality.

**Use Case:** Track export usage, enable re-download from cache, power export history UI.

**Implementation:**
```typescript
// After generating export
const exportRecord = await ctx.prisma.exportHistory.create({
  data: {
    userId: ctx.user.id,
    exportType: 'COMPLETE',  // or 'QUICK'
    format: 'ZIP',  // or 'CSV', 'JSON', 'EXCEL'
    dataType: null,  // null for COMPLETE, set for QUICK exports
    dateRange: null,  // { from: ISO, to: ISO } if date filtering used
    recordCount: totalRecords,  // Sum of all data type counts
    fileSize: zipBuffer.byteLength,
    blobKey: blobUrl,  // Vercel Blob URL
    expiresAt: new Date(Date.now() + 30 * 24 * 60 * 60 * 1000)  // 30 days
  }
})
```

**Query for Export History (UI endpoint):**
```typescript
// exports.getExportHistory procedure
const history = await ctx.prisma.exportHistory.findMany({
  where: { userId: ctx.user.id },
  orderBy: { createdAt: 'desc' },
  take: 10  // Last 10 exports
})

return history.map(exp => ({
  id: exp.id,
  type: exp.exportType,
  format: exp.format,
  dataType: exp.dataType,
  recordCount: exp.recordCount,
  fileSize: exp.fileSize,
  createdAt: exp.createdAt,
  expiresAt: exp.expiresAt,
  isExpired: exp.expiresAt < new Date(),
  blobKey: exp.blobKey
}))
```

**Recommendation:** Track all exports (both QUICK and COMPLETE) for analytics.

### Pattern 4: Vercel Cron Job Authentication

**Description:** Protect cron endpoints with CRON_SECRET verification (Bearer token).

**Use Case:** Cleanup cron job, any scheduled background tasks.

**Implementation (from existing generate-recurring cron):**
```typescript
// src/app/api/cron/cleanup-exports/route.ts
import { NextRequest, NextResponse } from 'next/server'
import { del } from '@vercel/blob'
import { prisma } from '@/lib/prisma'

export async function GET(request: NextRequest) {
  try {
    // Verify Vercel Cron authentication
    const authHeader = request.headers.get('authorization')
    const cronSecret = process.env.CRON_SECRET

    if (!cronSecret) {
      console.error('CRON_SECRET not configured')
      return NextResponse.json(
        { error: 'Cron configuration error' },
        { status: 500 }
      )
    }

    const expectedAuth = `Bearer ${cronSecret}`
    if (authHeader !== expectedAuth) {
      console.warn('Unauthorized cron request attempt')
      return NextResponse.json(
        { error: 'Unauthorized' },
        { status: 401 }
      )
    }

    // Find expired exports
    const expiredExports = await prisma.exportHistory.findMany({
      where: {
        expiresAt: { lt: new Date() }
      }
    })

    console.log(`Found ${expiredExports.length} expired exports to clean up`)

    // Delete from Vercel Blob Storage
    let deletedCount = 0
    let freedBytes = 0

    for (const exp of expiredExports) {
      if (exp.blobKey) {
        try {
          await del(exp.blobKey)
          deletedCount++
          freedBytes += exp.fileSize
        } catch (error) {
          console.error(`Failed to delete blob ${exp.blobKey}:`, error)
          // Continue with other deletions
        }
      }
    }

    // Delete from database
    await prisma.exportHistory.deleteMany({
      where: {
        id: { in: expiredExports.map(e => e.id) }
      }
    })

    console.log(`Cleanup complete: ${deletedCount} blobs deleted, ${freedBytes} bytes freed`)

    return NextResponse.json({
      success: true,
      message: 'Export cleanup completed',
      results: {
        exportsDeleted: expiredExports.length,
        blobsDeleted: deletedCount,
        bytesFreed: freedBytes
      },
      timestamp: new Date().toISOString()
    })
  } catch (error) {
    console.error('Error cleaning up exports:', error)
    return NextResponse.json(
      {
        success: false,
        error: error instanceof Error ? error.message : 'Unknown error',
        timestamp: new Date().toISOString()
      },
      { status: 500 }
    )
  }
}

// Support POST for manual triggering
export async function POST(request: NextRequest) {
  return GET(request)
}
```

**Recommendation:** Use this exact pattern from generate-recurring cron.

### Pattern 5: Export Summary Generation

**Description:** Generate summary.json with export metadata, record counts, and date ranges.

**Use Case:** Complete export package metadata file.

**Implementation:**
```typescript
interface SummaryInput {
  user: {
    email: string
    currency: string
    timezone: string
  }
  recordCounts: {
    transactions: number
    budgets: number
    goals: number
    accounts: number
    recurringTransactions: number
    categories: number
  }
  dateRange: {
    earliest: Date
    latest: Date
  } | null
  fileSize: number
}

export function generateSummary(input: SummaryInput): string {
  const summary = {
    exportVersion: '1.0',
    exportedAt: new Date().toISOString(),
    user: {
      email: input.user.email,
      currency: input.user.currency,
      timezone: input.user.timezone
    },
    recordCounts: input.recordCounts,
    dateRange: input.dateRange ? {
      earliest: format(input.dateRange.earliest, 'yyyy-MM-dd'),
      latest: format(input.dateRange.latest, 'yyyy-MM-dd')
    } : null,
    fileSize: input.fileSize,
    format: 'ZIP'
  }

  return JSON.stringify(summary, null, 2)
}
```

**Recommendation:** Create this utility in `src/lib/summaryGenerator.ts`.

## Complexity Assessment

### High Complexity Areas

**1. Vercel Blob Storage Integration (Builder-15-1)**
- Why complex: New third-party service, environment configuration, upload/download patterns, error handling for quota limits
- Estimated builder splits: 1 primary builder (sequential: setup → upload → download → cleanup)
- Risk mitigation: Test with small files first, implement graceful degradation if Blob unavailable
- Time estimate: 4-5 hours (setup: 1h, integration: 2h, testing: 1-2h)

**2. Complete Export Endpoint with ZIP Package (Builder-15-2)**
- Why complex: Coordinate 6 data fetches, generate 9 files, create ZIP, upload to Blob, record to history
- Estimated builder splits: 1 primary builder (can handle with sequential implementation)
- Risk mitigation: Reuse existing utilities (csvExport, xlsxExport, archiveExport), test with small datasets first
- Time estimate: 5-6 hours (endpoint: 2h, file generation: 2h, integration: 2h)

### Medium Complexity Areas

**3. Export History CRUD Operations (Builder-15-3)**
- Why complex: Multiple tRPC endpoints (getHistory, redownload, optional delete), query optimization, date formatting
- Time estimate: 3-4 hours (3 endpoints: 1h each, testing: 1h)
- Risk: Cache miss scenarios (blob deleted but DB record exists)

**4. Cleanup Cron Job Implementation (Builder-15-4)**
- Why complex: Vercel Cron configuration, authentication, Blob deletion, database cleanup, error handling
- Time estimate: 3-4 hours (cron route: 2h, vercel.json config: 30min, testing: 1-1.5h)
- Risk: Partial failures (some blobs delete, others fail) - need transaction-like handling

### Low Complexity Areas

**5. Summary.json Generator Utility**
- Why straightforward: Simple JSON generation with input data
- Time estimate: 1 hour

**6. Environment Variable Configuration**
- Why straightforward: Add BLOB_READ_WRITE_TOKEN to .env, update .env.example
- Time estimate: 30 minutes

## Technology Recommendations

### Primary Stack

**Cloud Storage: @vercel/blob@latest**
- Rationale: Free tier (1GB storage, 100GB bandwidth/month), Vercel-native, simple API, automatic CDN
- Alternative: AWS S3 (more complex setup, overkill for MVP)
- Installation: `npm install @vercel/blob`
- Configuration: Get BLOB_READ_WRITE_TOKEN from Vercel Dashboard → Storage → Create Blob Store

**Cron Scheduling: Vercel Cron (Built-in)**
- Rationale: No additional dependencies, integrated with Vercel deployment, simple configuration
- Alternative: External cron service (unnecessary complexity)
- Configuration: Add to `vercel.json` crons array

**Date Range Calculations: date-fns (Already Installed)**
- Rationale: Existing dependency, used throughout app, lightweight
- Use cases: Calculate earliest/latest transaction dates, format dates for summary.json

### Supporting Libraries

**None Required - All Needs Met:**
- ZIP generation: archiver (already installed in Iteration 14)
- CSV export: csvExport.ts utility (already complete)
- Excel export: xlsxExport.ts utility (already complete)
- AI context: aiContextGenerator.ts (already complete)
- README: readmeGenerator.ts (already complete)
- tRPC: @trpc/server (already installed)
- Prisma: @prisma/client (already installed)

### Configuration Requirements

**Environment Variables (.env and .env.example):**
```bash
# Vercel Blob Storage (NEW - Iteration 15)
BLOB_READ_WRITE_TOKEN="vercel_blob_rw_..."

# Already configured (verify)
CRON_SECRET="..."
DATABASE_URL="..."
DIRECT_URL="..."
```

**Vercel Configuration (vercel.json - UPDATE):**
```json
{
  "crons": [
    {
      "path": "/api/cron/generate-recurring",
      "schedule": "0 2 * * *"
    },
    {
      "path": "/api/cron/cleanup-exports",
      "schedule": "0 2 * * *"
    }
  ]
}
```

**Vercel Blob Setup Steps:**
1. Go to Vercel Dashboard → Project → Storage tab
2. Click "Create Database" → Select "Blob"
3. Name: "wealth-exports" (or similar)
4. Copy BLOB_READ_WRITE_TOKEN to .env
5. Deploy to apply environment variable

## Integration Points

### External APIs

**Vercel Blob Storage**
- Purpose: Cache complete export ZIP packages for 30-day re-download
- Complexity: Low (simple put/get/delete API)
- API Documentation: https://vercel.com/docs/storage/vercel-blob
- Considerations:
  - Free tier: 1GB storage (sufficient for ~500-1000 cached exports)
  - 100GB bandwidth/month (sufficient for moderate usage)
  - Automatic CDN distribution (fast global downloads)
  - Presigned URLs (secure, time-limited access)
  - Token rotation: Use Vercel environment variables (auto-rotated on redeploy)

**Code Examples:**
```typescript
import { put, del, list } from '@vercel/blob'

// Upload
const blob = await put(`exports/${userId}/complete-${timestamp}.zip`, zipBuffer, {
  access: 'public',
  contentType: 'application/zip'
})
// Returns: { url: 'https://...blob.vercel-storage.com/...', downloadUrl: '...', pathname: '...' }

// Delete
await del(blob.url)

// List (for debugging/admin)
const { blobs } = await list({ prefix: `exports/${userId}/` })
```

**Error Handling:**
- Quota exceeded: Log warning, return export without caching (direct download)
- Upload failure: Retry once, then fallback to direct download
- Delete failure: Log error, continue (will be cleaned up next cron run)

### Internal Integrations

**tRPC Router ↔ Export Utilities**
- Data flow: Client → tRPC mutation → Prisma queries → Export utilities → ZIP generation → Blob upload → Response
- Connection points:
  - `exports.exportComplete` calls: csvExport, xlsxExport, aiContextGenerator, readmeGenerator, summaryGenerator, archiveExport
  - Each utility returns string/Buffer content
  - archiveExport combines all files into ZIP Buffer
  - Blob Storage receives ZIP Buffer, returns URL
  - ExportHistory records metadata

**Export History ↔ Blob Storage**
- Relationship: ExportHistory.blobKey stores Vercel Blob URL
- Lifecycle:
  1. Export created → Blob uploaded → blobKey stored in DB
  2. User requests re-download → Query ExportHistory → Return blobKey URL
  3. Export expires (30 days) → Cron deletes Blob → Deletes DB record
- Cache miss handling:
  - If blobKey exists but Blob deleted: Return error, offer "Generate Fresh Export"
  - If expiresAt < now: Return error, show "Expired - Generate Fresh"

**Cleanup Cron ↔ ExportHistory ↔ Blob Storage**
- Trigger: Vercel Cron (daily at 2 AM UTC)
- Process:
  1. Query ExportHistory where expiresAt < now()
  2. For each record: Delete from Blob Storage (if blobKey exists)
  3. Delete all expired records from DB
  4. Log results (count, bytes freed)
- Error handling:
  - Individual Blob delete failure: Log, continue with others
  - Database delete failure: Critical, throw error (retry next run)

## Risks & Challenges

### Technical Risks

**1. Vercel Blob Storage Quota Exhaustion**
- Risk: Free tier 1GB could fill up with large exports (2-5MB each)
- Impact: Medium (export caching stops working, users see "Generate Fresh" only)
- Likelihood: Low-Medium (depends on user adoption)
- Mitigation Strategy:
  - Monitor storage usage via Vercel Dashboard → Storage tab
  - 30-day expiration ensures automatic cleanup
  - Graceful degradation: If Blob upload fails, return export without caching (direct download still works)
  - Upgrade path: Vercel Pro ($20/month) provides 100GB storage
- Monitoring:
  ```typescript
  // Add to admin dashboard (future)
  const { blobs } = await list()
  const totalSize = blobs.reduce((sum, b) => sum + b.size, 0)
  console.log(`Blob storage used: ${(totalSize / 1024 / 1024 / 1024).toFixed(2)} GB / 1 GB`)
  ```

**2. Large Export Memory Overflow**
- Risk: Users with 50k+ transactions generate 10-20MB ZIP in memory
- Impact: High (server crashes, export failures)
- Likelihood: Low (transaction limit enforced at 10k, warning at 5k)
- Mitigation Strategy:
  - Hard limit already in place (exports.router.ts checks record count)
  - archiver uses streaming (doesn't load full ZIP into memory)
  - Vercel serverless function max memory: 1GB (sufficient for 20MB exports)
  - Monitor export duration (timeout after 30s)

**3. Blob Delete Failures in Cleanup Cron**
- Risk: Blob deletion API call fails (network error, rate limit, etc.)
- Impact: Medium (orphaned blobs accumulate, storage quota fills)
- Likelihood: Low (Blob API is reliable)
- Mitigation Strategy:
  - Continue with other deletions on individual failures
  - Log failures for manual review
  - Database records remain until Blob successfully deleted (prevent orphaning)
  - Manual cleanup endpoint for admins (future)
- Code pattern:
  ```typescript
  for (const exp of expiredExports) {
    if (exp.blobKey) {
      try {
        await del(exp.blobKey)
      } catch (error) {
        console.error(`Failed to delete blob ${exp.blobKey}:`, error)
        // Continue with others, leave DB record (will retry next run)
        continue
      }
    }
  }
  // Only delete from DB if Blob deletion succeeded
  await prisma.exportHistory.deleteMany({
    where: { id: { in: successfullyDeletedIds } }
  })
  ```

### Complexity Risks

**4. Complete Export Endpoint Coordination**
- Risk: Complex orchestration of 6 data fetches + 9 file generations + ZIP creation + Blob upload + DB recording
- Likelihood: Medium (many moving parts)
- Mitigation: Break into clear sequential steps, add timing logs, test with small datasets first
- Debugging strategy:
  1. Test each data fetch independently
  2. Test each file generator (CSV, JSON, AI context, README, summary)
  3. Test ZIP creation with sample files
  4. Test Blob upload with sample ZIP
  5. Integrate incrementally
- Time-box: If >2 hours debugging, escalate to planner

**5. Export History Cache Miss Scenarios**
- Risk: User requests re-download but Blob was manually deleted or expired
- Likelihood: Low (automatic cleanup handles expiration)
- Mitigation: Check Blob existence before returning download URL
- UX handling:
  - If expiresAt < now: Show "Expired" badge, disable re-download, offer "Generate Fresh"
  - If blobKey null: Show "Generate Fresh" only
  - If Blob 404: Log warning, delete DB record, offer "Generate Fresh"

## Recommendations for Planner

### 1. Install @vercel/blob FIRST (Before Any Code)
Vercel Blob is the foundation for Iteration 15. Install and configure before building endpoints.

**Action:**
```bash
npm install @vercel/blob
# Add BLOB_READ_WRITE_TOKEN to .env (get from Vercel Dashboard)
# Update .env.example with token documentation
```

**Verification:**
```typescript
// Quick test in any file
import { put } from '@vercel/blob'
const testBlob = await put('test.txt', 'Hello Vercel Blob', { access: 'public' })
console.log('Blob URL:', testBlob.url)
```

### 2. Reuse Iteration 14 Utilities (Don't Rebuild)
All core export utilities are complete and tested:
- csvExport.ts (6 data types)
- xlsxExport.ts (6 data types)
- aiContextGenerator.ts
- readmeGenerator.ts
- archiveExport.ts

**Action:** Builder should import and use these utilities, not rewrite logic.

### 3. Follow Existing Cron Pattern (generate-recurring)
The cleanup cron job should mirror `src/app/api/cron/generate-recurring/route.ts` exactly:
- Same authentication pattern (Bearer token)
- Same error handling structure
- Same response format
- Same logging approach

**Action:** Copy generate-recurring cron skeleton, replace logic with cleanup code.

### 4. Add Summary Generator Utility (Missing)
Iteration 14 didn't include summaryGenerator.ts (oversight). Needs to be created.

**Action:** Create `src/lib/summaryGenerator.ts` following pattern in this report.

### 5. Graceful Degradation for Blob Failures
If Blob upload fails, exports should still work (direct download without caching).

**Pattern:**
```typescript
let blobKey: string | null = null
try {
  const blob = await put(path, zipBuffer, options)
  blobKey = blob.url
} catch (error) {
  console.error('Blob upload failed, export will not be cached:', error)
  // Continue without caching (user gets direct download)
}

await prisma.exportHistory.create({
  data: {
    // ... other fields ...
    blobKey,  // Will be null if upload failed
  }
})
```

### 6. Monitor Export Performance from Day 1
Add timing logs to all export endpoints:
```typescript
const startTime = Date.now()
// ... export generation ...
const duration = Date.now() - startTime
console.log(`Complete export: ${duration}ms, ${recordCount} total records, ${fileSize} bytes`)
```

**Action:** Include performance logging in all tRPC procedures.

### 7. Test Cleanup Cron Locally Before Deployment
Vercel Cron can't be tested locally. Create manual test endpoint:
```typescript
// src/app/api/test/cleanup-exports/route.ts (dev only)
export async function GET() {
  // Same logic as cron, but no auth check
  // Delete after testing
}
```

**Action:** Test cleanup logic before deploying to production.

### 8. Update .env.example with Blob Token Documentation
Help future developers set up Vercel Blob correctly.

**Action:**
```bash
# .env.example
# Vercel Blob Storage (REQUIRED for export caching)
# Get from: Vercel Dashboard → Storage → Create Blob Store → Copy token
BLOB_READ_WRITE_TOKEN="vercel_blob_rw_..."
```

## Resource Map

### Critical Files/Directories

**Existing (Already Complete - DO NOT MODIFY)**
- `src/lib/csvExport.ts` - All 6 CSV generators (working perfectly)
- `src/lib/xlsxExport.ts` - All 6 Excel generators (tested)
- `src/lib/aiContextGenerator.ts` - AI context JSON generator
- `src/lib/readmeGenerator.ts` - README template generator
- `src/lib/archiveExport.ts` - ZIP package creator
- `src/server/api/routers/exports.router.ts` - 6 Quick export endpoints (transactions, budgets, etc.)
- `prisma/schema.prisma` - ExportHistory model (migration applied)
- `src/lib/__tests__/*.test.ts` - Export utility tests (all passing)

**New (Create in Iteration 15)**
- `src/lib/summaryGenerator.ts` - Generate summary.json for ZIP packages (MISSING from Iteration 14)
- `src/app/api/cron/cleanup-exports/route.ts` - Vercel Cron cleanup job
- `src/server/api/routers/exports.router.ts` - ADD 3 new procedures:
  - `exportComplete` - Generate full ZIP package
  - `getExportHistory` - List user's past exports
  - `redownloadExport` - Re-download cached export from Blob

**Modify (Iteration 15)**
- `vercel.json` - Add cleanup cron job configuration
- `.env.example` - Add BLOB_READ_WRITE_TOKEN documentation
- `.env` - Add BLOB_READ_WRITE_TOKEN value (get from Vercel Dashboard)

**Testing (Optional - Manual Testing Sufficient)**
- `src/server/api/routers/__tests__/exports.router.test.ts` - Add tests for new endpoints
- `src/lib/__tests__/summaryGenerator.test.ts` - Test summary.json generation

### Key Dependencies

**Install in Iteration 15:**
```bash
npm install @vercel/blob
```

**Already Installed (Verify):**
- archiver@7.0.1 (ZIP generation)
- @types/archiver@7.0.0 (TypeScript support)
- xlsx@0.18.5 (Excel generation)
- date-fns@3.6.0 (Date formatting)
- @prisma/client@5.22.0 (Database ORM)
- @trpc/server@11.6.0 (API layer)
- zod@3.23.8 (Input validation)

### Testing Infrastructure

**Vitest Configuration (Already Set Up)**
- Config file: `vitest.config.ts`
- Globals enabled (no import needed)
- Node environment
- Coverage with v8

**Test Commands:**
```bash
npm run test                 # Run all tests
npm run test:ui              # Visual test UI
npm run test:coverage        # Coverage report
```

**Test Priorities (Iteration 15):**
1. Manual testing: Generate complete export, verify ZIP structure
2. Manual testing: Upload to Vercel Blob, verify download URL works
3. Manual testing: Trigger cleanup cron (via test endpoint), verify DB and Blob cleanup
4. Automated tests: Optional (defer to post-MVP if time-constrained)

## Questions for Planner

### 1. Should we cache Quick exports (CSV/JSON/Excel) or only Complete ZIP packages?

**Context:** Iteration 14 added individual data type exports (Quick exports). Do we cache these to Blob Storage?

**Options:**
- A) Cache all exports (Quick + Complete)
- B) Cache Complete exports only (ZIP packages)
- C) Cache exports >1MB only (size threshold)

**Recommendation:** Option B (Complete exports only). Rationale:
- Quick exports are fast to regenerate (<3s)
- Complete exports are slow (10-15s for large datasets)
- Storage quota preservation (1GB free tier)
- Simpler implementation (one cache strategy)

**Impact:** Reduces Blob Storage usage by ~70%, simplifies caching logic.

---

### 2. What's the maximum file size we should allow for exports?

**Context:** Complete exports with 10k transactions can reach 5-10MB. Should we limit?

**Options:**
- A) No limit (trust 10k transaction limit)
- B) Warn at 5MB: "Large export may take 60 seconds"
- C) Hard limit at 10MB: "Use date filters to reduce size"

**Recommendation:** Option B (warning only). Rationale:
- 10k transaction limit already prevents extreme sizes
- Users with large datasets need complete exports
- Warning sets expectations (UX)
- archiver handles large ZIPs well

**Impact:** Better UX, no blocking limits.

---

### 3. Should export history show failed/partial exports?

**Context:** ExportHistory currently tracks successful exports only. Track failures?

**Options:**
- A) Track all (add `status` field: PENDING, SUCCESS, FAILED)
- B) Track successful only (current approach)
- C) Track successful + failed (no pending state)

**Recommendation:** Option B (successful only) for Iteration 15. Add status field in post-MVP if analytics show value.

**Impact:** Simpler iteration 15, defer complexity.

---

### 4. How should we handle Blob Storage quota exhaustion?

**Context:** Free tier is 1GB. What happens when full?

**Options:**
- A) Block new exports: "Storage quota exceeded, contact support"
- B) Delete oldest cached exports: FIFO eviction
- C) Graceful degradation: Return uncached exports (direct download)

**Recommendation:** Option C (graceful degradation). Rationale:
- Exports still work (core functionality maintained)
- Users get "Generate Fresh" for all exports (no re-download)
- Admin can manually clear old exports or upgrade tier
- Best UX (no blocking errors)

**Implementation:**
```typescript
try {
  const blob = await put(path, buffer, options)
  blobKey = blob.url
} catch (error) {
  if (error.message.includes('quota')) {
    console.error('Blob quota exceeded, export not cached')
    // Return export without caching
  }
}
```

**Impact:** Prevents user-facing errors, enables monitoring/alerting.

---

### 5. Should cleanup cron also delete old ExportHistory records (beyond 30 days) even if not cached?

**Context:** Some exports might not be cached (Blob upload failed). Should we delete DB records after 30 days regardless?

**Options:**
- A) Delete all ExportHistory records after 30 days (cached or not)
- B) Delete only cached exports (leave uncached records as historical log)
- C) Delete cached exports after 30 days, uncached after 90 days

**Recommendation:** Option A (delete all after 30 days). Rationale:
- Consistent expiration policy
- Prevents database bloat
- Analytics can be extracted before deletion (future admin dashboard)
- Users don't expect re-download past 30 days

**Impact:** Cleaner database, consistent UX.

---

### 6. Should we support manual export deletion (user-initiated cleanup)?

**Context:** Users might want to delete exports before 30-day expiration (privacy).

**Options:**
- A) Yes, add "Delete" button in export history UI
- B) No, automatic cleanup only (simpler)
- C) Admin only (manual cleanup endpoint for support)

**Recommendation:** Option B (automatic only) for Iteration 15. Add manual delete in post-MVP if users request it.

**Impact:** Simpler iteration 15, defer to user feedback.

---

## Limitations

**MCP Servers: Not Used**
This exploration did not utilize MCP servers (Playwright, Chrome DevTools, Supabase Local) as Iteration 15 focuses on backend integration patterns and Vercel Blob Storage setup. Future validation could leverage:
- Playwright MCP: E2E testing of complete export flow (click → generate → download → re-download)
- Chrome DevTools MCP: Performance profiling of large ZIP generation (10k+ transactions)
- Supabase Local MCP: Verify ExportHistory queries use indexes efficiently

**Recommendation:** Manual testing sufficient for Iteration 15 (generate export, verify ZIP structure, test re-download, trigger cleanup cron). Defer automated E2E tests to post-MVP.

---

**Report Complete**
Explorer-2 analysis for Iteration 15 finished. Ready for planner synthesis.
